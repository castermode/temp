"""
ä½¿ç”¨ mem0 1.0.0 æ·»åŠ è®°å¿†çš„ç¤ºä¾‹ä»£ç 
ä½¿ç”¨ Azure OpenAI ä½œä¸º LLM å’Œ Embedding æ¨¡å‹
è¿è¡Œç¯å¢ƒä¸º conda activate py311
"""

import argparse
import json
import os
from mem0 import Memory


# ä¿®æ”¹ä»£ç ï¼Œé€šè¿‡å‘½ä»¤è¡Œå‚æ•° --infer å†³å®šæ˜¯å¦å¯ç”¨addæ–¹æ³•çš„inferå‚æ•°
# å¦‚æœæœ‰ --infer å‚æ•°ï¼Œåˆ™è®¾ç½®addæ–¹æ³•çš„inferå‚æ•°ä¸ºTrue
# å¦‚æœæ²¡æœ‰ --infer å‚æ•°ï¼Œåˆ™è®¾ç½®addæ–¹æ³•çš„inferå‚æ•°ä¸ºFalse

os.environ["MEM0_TELEMETRY"] = "false"

# è§£æå‘½ä»¤è¡Œå‚æ•°
parser = argparse.ArgumentParser(
    description='KeMem è®°å¿†ç®¡ç†æµ‹è¯•å·¥å…·',
    formatter_class=argparse.RawDescriptionHelpFormatter,
    epilog='''
ä½¿ç”¨ç¤ºä¾‹:
  # ä½¿ç”¨é»˜è®¤é…ç½®ï¼ˆä»…å‘é‡å­˜å‚¨ï¼‰
  python kemem_test.py
  
  # å¯ç”¨å›¾æ•°æ®åº“
  python kemem_test.py --graph
  
  # å¯ç”¨ infer å‚æ•°ï¼ˆæ¨ç†è®°å¿†å†…å®¹ï¼‰
  python kemem_test.py --infer
  
  # æŒ‡å®šå¯¹è¯æ–‡ä»¶
  python kemem_test.py --chats ./chats.txt
  
  # ä½¿ç”¨è‡ªå®šä¹‰ update_memory_prompt
  python kemem_test.py --update-memory-prompt ./my_update_prompt.txt
  
  # æŒ‡å®šç”¨æˆ· ID
  python kemem_test.py --user-id user_002
  
  # ç»„åˆä½¿ç”¨ï¼ˆå¯ç”¨å›¾æ•°æ®åº“ + infer + è‡ªå®šä¹‰é…ç½®ï¼‰
  python kemem_test.py --graph --infer --chats ./chats.txt --update-memory-prompt ./my_update_prompt.txt --user-id user_002
    '''
)

parser.add_argument(
    '--update-memory-prompt',
    type=str,
    default=None,
    metavar='FILE',
    help='è‡ªå®šä¹‰ update_memory_prompt æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ä½¿ç”¨ Mem0 å†…ç½® promptï¼‰'
)

parser.add_argument(
    '--user-id',
    type=str,
    default='user_001',
    metavar='ID',
    help='ç”¨æˆ· IDï¼Œç”¨äºéš”ç¦»ä¸åŒç”¨æˆ·çš„è®°å¿†ï¼ˆé»˜è®¤: user_001ï¼‰'
)

parser.add_argument(
    '--chats',
    type=str,
    default='chats.txt',
    metavar='FILE',
    help='åŒ…å«å¯¹è¯è®°å½•çš„æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: chats.txtï¼‰'
)

parser.add_argument(
    '--graph',
    action='store_true',
    help='å¯ç”¨å›¾æ•°æ®åº“ï¼ˆKuzuï¼‰æ¥å­˜å‚¨å®ä½“å…³ç³»'
)

parser.add_argument(
    '--infer',
    action='store_true',
    help='å¯ç”¨ add æ–¹æ³•çš„ infer å‚æ•°ï¼Œç”¨äºæ¨ç†è®°å¿†å†…å®¹'
)

args = parser.parse_args()

# è¯»å–è‡ªå®šä¹‰ promptï¼ˆå¦‚æœæä¾›ï¼‰
MY_UPDATE_PROMPT = None
if args.update_memory_prompt:
    try:
        with open(args.update_memory_prompt, 'r', encoding='utf-8') as f:
            MY_UPDATE_PROMPT = f.read()
        print(f"å·²åŠ è½½è‡ªå®šä¹‰ update_memory_prompt: {args.update_memory_prompt}")
    except FileNotFoundError:
        print(f"è­¦å‘Šï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {args.update_memory_prompt}ï¼Œå°†ä½¿ç”¨é»˜è®¤ prompt")
    except Exception as e:
        print(f"è­¦å‘Šï¼šè¯»å–æ–‡ä»¶æ—¶å‡ºé”™ {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤ prompt")

#é…ç½® Azure OpenAI æ¨¡å‹
config = {
#LLM é…ç½® - gpt - 4.1 - nano
    "llm": {
        "provider": "azure_openai",
        "config": {
            "model": "gpt-4.1-nano",
            "azure_kwargs": {
                "api_key": os.getenv("GPT_41_NANO_KEY"),
                "azure_deployment": "gpt-4.1-nano",
                "azure_endpoint": "https://bk-us-2.openai.azure.com",
                "api_version": "2025-01-01-preview",
            }
        }
    },
#Embedding é…ç½® - text - embedding - 3 - small
    "embedder": {
        "provider": "azure_openai",
        "config": {
            "model": "text-embedding-3-small",
            "embedding_dims": 1536,  # text-embedding-3-small çš„é»˜è®¤ç»´åº¦
            "azure_kwargs": {
                "api_key": os.getenv("TEXT_EMBEDDING_3_SMALL"),
                "azure_deployment": "text-embedding-3-small",
                "azure_endpoint": "https://bk-cloud.openai.azure.com",
                "api_version": "2023-05-15",
            }
        }
    },
# å‘é‡å­˜å‚¨é…ç½® - æ˜ç¡®æŒ‡å®šå­˜å‚¨è·¯å¾„
    "vector_store": {
        "provider": "qdrant",
        "config": {
            "path": "./memorydb/vector",  # ä½¿ç”¨æœ¬åœ°ç›®å½•å­˜å‚¨å‘é‡æ•°æ®
            "on_disk": True  # å¯ç”¨æŒä¹…åŒ–å­˜å‚¨ï¼Œé˜²æ­¢æ¯æ¬¡åˆå§‹åŒ–æ—¶åˆ é™¤æ•°æ®
        }
    }
}

# å¦‚æœæä¾›äº†è‡ªå®šä¹‰ promptï¼Œåˆ™æ·»åŠ åˆ°é…ç½®ä¸­
if MY_UPDATE_PROMPT:
    config["custom_update_memory_prompt"] = MY_UPDATE_PROMPT
    print("ä½¿ç”¨è‡ªå®šä¹‰ update_memory_prompt")
else:
    print("ä½¿ç”¨é»˜è®¤ update_memory_prompt")

# å¦‚æœå¯ç”¨äº†å›¾æ•°æ®åº“ï¼Œåˆ™æ·»åŠ åˆ°é…ç½®ä¸­
if args.graph:
    config["graph_store"] = {
        "provider": "kuzu",
        "config": {
            "db": "./memorydb/graph/kemem_graph.db"  # ä½¿ç”¨æœ¬åœ°æ–‡ä»¶å­˜å‚¨å›¾æ•°æ®
        }
    }
    print("âœ… å·²å¯ç”¨å›¾æ•°æ®åº“ï¼ˆKuzuï¼‰")
else:
    print("âšª æœªå¯ç”¨å›¾æ•°æ®åº“ï¼ˆä»…ä½¿ç”¨å‘é‡å­˜å‚¨ï¼‰")

#åˆ›å»º Memory å®ä¾‹
print("åˆ›å»º Memory å®ä¾‹...")
memory = Memory.from_config(config_dict=config)

# æ˜¾ç¤ºæ˜¯å¦å¯ç”¨äº†å›¾æ•°æ®åº“
if hasattr(memory, 'enable_graph'):
    print(f"å›¾æ•°æ®åº“çŠ¶æ€: {memory.enable_graph}")


# ä»å¤–éƒ¨æ–‡ä»¶ chats.txt ä¸­è¯»å–å¯¹è¯è®°å½•å¹¶æ·»åŠ åˆ°è®°å¿†ä¸­
# chats.txt æ¯è¡Œä¸€æ¡è®°å¿†ï¼ˆJSON æ ¼å¼ï¼‰

print(f"\nä»æ–‡ä»¶è¯»å–å¯¹è¯è®°å½•: {args.chats}")
print("=" * 60)

# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
if not os.path.exists(args.chats):
    print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {args.chats}")
    print("è¯·ç¡®ä¿ chats.txt æ–‡ä»¶å­˜åœ¨äºå½“å‰ç›®å½•")
    exit(1)

# è¯»å–å¹¶å¤„ç†æ¯ä¸€è¡Œå¯¹è¯è®°å½•
try:
    with open(args.chats, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    print(f"å…±è¯»å–åˆ° {len(lines)} æ¡å¯¹è¯è®°å½•\n")
    
    for idx, line in enumerate(lines, 1):
        line = line.strip()
        if not line:  # è·³è¿‡ç©ºè¡Œ
            continue
        
        try:
            # è§£æ JSON æ ¼å¼çš„å¯¹è¯
            messages = json.loads(line)
            
            print(f"[{idx}/{len(lines)}] æ·»åŠ è®°å¿†ï¼š")
            # æ˜¾ç¤ºå¯¹è¯å†…å®¹
            for msg in messages:
                role = msg.get('role', '')
                content = msg.get('content', '')
                print(f"  {role}: {content}")
            
            # æ·»åŠ åˆ°è®°å¿†ä¸­
            result = memory.add(
                messages=messages,
                user_id=args.user_id,
                infer=args.infer
            )
            
            # å¦‚æœå¯ç”¨äº†å›¾æ•°æ®åº“ï¼Œæ˜¾ç¤ºå›¾å…³ç³»ä¿¡æ¯
            if args.graph and result.get('relations'):
                relations = result['relations']
                if isinstance(relations, dict):
                    added = relations.get('added_entities', [])
                    if added:
                        print(f"\n  ğŸ”— æ–°å¢å›¾å…³ç³»: {len(added)} æ¡")
                        for rel in added[:5]:
                            if isinstance(rel, dict):
                                print(f"     {rel.get('source')} --[{rel.get('relationship')}]--> {rel.get('target')}")
            
            # æ¯æ¬¡æ·»åŠ åæŸ¥çœ‹æ‰€æœ‰è®°å¿†
            print(f"\n  å½“å‰æ‰€æœ‰è®°å¿†ï¼š")
            all_memories = memory.get_all(user_id=args.user_id)
            if all_memories and all_memories.get('results'):
                for mem in all_memories['results']:
                    print(f"    - [{mem['id'][:8]}...] {mem['memory']}")
            else:
                print("    (æ— è®°å¿†)")
            print("-" * 60)
            
        except json.JSONDecodeError as e:
            print(f"  è­¦å‘Šï¼šç¬¬ {idx} è¡Œ JSON è§£æå¤±è´¥: {e}")
            print(f"  å†…å®¹: {line}")
            print("-" * 60)
            continue
        except Exception as e:
            print(f"  é”™è¯¯ï¼šæ·»åŠ ç¬¬ {idx} è¡Œè®°å¿†æ—¶å‡ºé”™: {e}")
            print("-" * 60)
            continue
    
    print("\næ‰€æœ‰å¯¹è¯è®°å½•å¤„ç†å®Œæˆï¼")
    print("=" * 60)
    
except Exception as e:
    print(f"é”™è¯¯ï¼šè¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    exit(1)










# æœç´¢è®°å¿†
print("\n\n" + "=" * 60)
print("å¼€å§‹æœç´¢è®°å¿†")
print("=" * 60)

queries = [
    "æˆ‘å–œæ¬¢åƒä»€ä¹ˆ",
    "æˆ‘å–œæ¬¢å¹²ä»€ä¹ˆ",
    "å…³äºè‹¹æœ"
]

for query_msg in queries:
    print(f"\næœç´¢æŸ¥è¯¢ï¼š{query_msg}")
    print("-" * 40)
    result = memory.search(
        query=query_msg,
        user_id=args.user_id
    )
    
    # æ ¼å¼åŒ–è¾“å‡ºæœç´¢ç»“æœ
    if result and result.get('results'):
        print(f"æ‰¾åˆ° {len(result['results'])} æ¡ç›¸å…³è®°å¿†ï¼š")
        for idx, mem in enumerate(result['results'], 1):
            print(f"  {idx}. {mem.get('memory', 'N/A')}")
            if 'score' in mem:
                print(f"     (ç›¸å…³åº¦: {mem['score']:.4f})")
    else:
        print("  æœªæ‰¾åˆ°ç›¸å…³è®°å¿†")
    
    # å¦‚æœå¯ç”¨äº†å›¾æ•°æ®åº“ï¼Œæ˜¾ç¤ºå›¾å…³ç³»
    if args.graph and result.get('relations'):
        relations = result['relations']
        if isinstance(relations, list) and relations:
            print(f"\n  ğŸ”— ç›¸å…³å›¾å…³ç³»: {len(relations)} æ¡")
            for rel in relations[:5]:
                if isinstance(rel, dict):
                    source = rel.get('source', 'N/A')
                    relationship = rel.get('relationship', rel.get('relation', 'N/A'))
                    destination = rel.get('destination', rel.get('target', 'N/A'))
                    print(f"     {source} --[{relationship}]--> {destination}")
    
    print()

print("=" * 60)
print("æµ‹è¯•å®Œæˆï¼")
print("=" * 60)